{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0KEpC74+wSJCIqwX/Vga0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl96OvXDJUs3",
        "outputId": "80a5e23f-8d8b-4b5c-ac3d-e27da6a7ede4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m568.9/797.2 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:09\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124 -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install peft transformers protobuf==3.20 bitsandbytes sentencepiece sacrebleu ipython datasets evaluate deepspeed einops  wandb zstandard accelerate jsonlines trl -q"
      ],
      "metadata": {
        "id": "6uNJryktJWT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "from peft import PeftModel"
      ],
      "metadata": {
        "id": "QIQleojAJYb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://huggingface.co/haoranxu/ALMA-7B /home/jupyter/datasphere/project/models"
      ],
      "metadata": {
        "id": "OYl-qxL0JbAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"haoranxu/X-ALMA-13B-Pretrain\", torch_dtype=torch.float16, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"haoranxu/X-ALMA-13B-Pretrain\", padding_side='left')"
      ],
      "metadata": {
        "id": "calqchQTJdXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the source sentence into the prompt template\n",
        "prompt=\"Translate this from English to Russian:\\nEnglish: Hello, how are you!\\nRussian:\"\n",
        "\n",
        "# X-ALMA needs chat template but ALMA and ALMA-R don't need it.\n",
        "# chat_style_prompt = [{\"role\": \"user\", \"content\": prompt}]\n",
        "# prompt = tokenizer.apply_chat_template(chat_style_prompt, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True, max_length=40, truncation=True).input_ids.cuda()\n",
        "\n",
        "# Translation\n",
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(input_ids=input_ids, num_beams=5, max_new_tokens=20, do_sample=True, temperature=0.8, top_p=0.9)\n",
        "outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "c9CXOeyyJff_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "Ikc1gHuKJjc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pyyaml -q"
      ],
      "metadata": {
        "id": "towcLak2Jl1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml"
      ],
      "metadata": {
        "id": "UBSojo4WJoOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %run ./ALMA/run_cpo_llmmt.py \\\n",
        "#     --config_name /home/jupyter/datasphere/project/ALMA/configs/conf.json \\\n",
        "#     --model_name_or_path haoranxu/X-ALMA-13B-Pretrain \\\n",
        "#     --tokenizer_name haoranxu/X-ALMA-13B-Pretrain \\\n",
        "#     --peft_model_id  haoranxu/ALMA-13B-Pretrain-LoRA \\\n",
        "#     --cpo_scorer kiwi_xcomet \\\n",
        "#     --beta 0.1 \\\n",
        "#     --use_peft \\\n",
        "#     --use_fast_tokenizer False \\\n",
        "#     --cpo_data_path  \"marulyanova/ALMA-R-Preference-F\" \\\n",
        "#     --do_train \\\n",
        "#     --language_pairs \"['ru-en']\" \\\n",
        "#     --low_cpu_mem_usage \\\n",
        "#     --bf16 \\\n",
        "#     --learning_rate 1e-4 \\\n",
        "#     --weight_decay 0.01 \\\n",
        "#     --gradient_accumulation_steps 1 \\\n",
        "#     --lr_scheduler_type inverse_sqrt \\\n",
        "#     --warmup_ratio 0.01 \\\n",
        "#     --ignore_pad_token_for_loss \\\n",
        "#     --ignore_prompt_token_for_loss \\\n",
        "#     --per_device_train_batch_size 2 \\\n",
        "#     --evaluation_strategy no \\\n",
        "#     --save_strategy steps \\\n",
        "#     --save_total_limit 1 \\\n",
        "#     --save_steps=150 \\\n",
        "#     --logging_strategy steps \\\n",
        "#     --logging_steps 0.05 \\\n",
        "#     --output_dir dist \\\n",
        "#     --num_train_epochs 1 \\\n",
        "#     --prediction_loss_only \\\n",
        "#     --max_new_tokens 256 \\\n",
        "#     --max_source_length 256 \\\n",
        "#     --max_prompt_length 256 \\\n",
        "#     --max_length 512 \\\n",
        "#     --seed 42 \\\n",
        "#     --overwrite_output_dir \\\n",
        "#     --report_to none \\\n",
        "#     --overwrite_cache"
      ],
      "metadata": {
        "id": "oTW2Ff66JqtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --config_name ALMA/configs/conf.json \\"
      ],
      "metadata": {
        "id": "GGg3D7lmJtpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%run ./ALMA/run_cpo_llmmt.py \\\n",
        "    --config_name ALMA/configs/conf.json \\\n",
        "    --model_name_or_path haoranxu/X-ALMA-13B-Pretrain \\\n",
        "    --tokenizer_name haoranxu/X-ALMA-13B-Pretrain \\\n",
        "    --peft_model_id  haoranxu/ALMA-13B-Pretrain-LoRA \\\n",
        "    --cpo_scorer kiwi_xcomet \\\n",
        "    --beta 0.1 \\\n",
        "    --use_peft \\\n",
        "    --use_fast_tokenizer False \\\n",
        "    --cpo_data_path  marulyanova/ALMA-R-Preference-F \\\n",
        "    --do_train \\\n",
        "    --language_pairs \"['ru-en']\" \\\n",
        "    --low_cpu_mem_usage \\\n",
        "    --fp16 \\\n",
        "    --learning_rate 1e-4 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --lr_scheduler_type inverse_sqrt \\\n",
        "    --warmup_ratio 0.01 \\\n",
        "    --ignore_pad_token_for_loss \\\n",
        "    --ignore_prompt_token_for_loss \\\n",
        "    --per_device_train_batch_size 1 \\\n",
        "    --evaluation_strategy no \\\n",
        "    --save_strategy steps \\\n",
        "    --save_total_limit 1 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 0.05 \\\n",
        "    --output_dir dist \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --prediction_loss_only \\\n",
        "    --max_new_tokens 256 \\\n",
        "    --max_source_length 256 \\\n",
        "    --max_prompt_length 256 \\\n",
        "    --max_length 512 \\\n",
        "    --seed 42 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --report_to none \\\n",
        "    --overwrite_cache \\\n",
        "    --eval_accumulation_steps 1 \\\n",
        "    --per_device_eval_batch_size 1"
      ],
      "metadata": {
        "id": "0kgOqJbsJwla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torch transformers --upgrade -q"
      ],
      "metadata": {
        "id": "41R-qlFpJ3Gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/fe1ixxu/ALMA.git"
      ],
      "metadata": {
        "id": "J4fVveGVJ97Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Определяем конфигурацию в виде словаря\n",
        "config = {\n",
        "    \"compute_environment\": \"LOCAL_MACHINE\",\n",
        "    \"deepspeed_config\": {\n",
        "        \"gradient_accumulation_steps\": 1,\n",
        "        \"gradient_clipping\": 1.0,\n",
        "        \"offload_optimizer_device\": \"none\",\n",
        "        \"offload_param_device\": \"cpu\",\n",
        "        \"zero3_init_flag\": False,\n",
        "        \"zero_stage\": 2\n",
        "    },\n",
        "    \"distributed_type\": \"DEEPSPEED\",\n",
        "    \"downcast_bf16\": \"no\",\n",
        "    \"machine_rank\": 0,\n",
        "    \"main_training_function\": \"main\",\n",
        "    \"mixed_precision\": \"bf16\",\n",
        "    \"num_machines\": 1,\n",
        "    \"num_processes\": 8,\n",
        "    \"rdzv_backend\": \"static\",\n",
        "    \"same_network\": True,\n",
        "    \"tpu_env\": [],\n",
        "    \"tpu_use_cluster\": False,\n",
        "    \"tpu_use_sudo\": False,\n",
        "    \"use_cpu\": False,\n",
        "    \"model_type\": \"llama\",\n",
        "    \"max_position_embeddings\": 4096,\n",
        "    \"attention_bias\": False,\n",
        "    \"attention_dropout\": 0.0,\n",
        "    \"bos_token_id\": 1,\n",
        "    \"eos_token_id\": 2,\n",
        "    \"head_dim\": 128,\n",
        "    \"hidden_act\": \"silu\",\n",
        "    \"hidden_size\": 5120,\n",
        "    \"initializer_range\": 0.02,\n",
        "    \"intermediate_size\": 13824,\n",
        "    \"max_length\": 512,\n",
        "    \"max_position_embeddings\": 4096,\n",
        "    \"mlp_bias\": False,\n",
        "    \"model_type\": \"llama\",\n",
        "    \"num_attention_heads\": 40,\n",
        "    \"num_hidden_layers\": 40,\n",
        "    \"num_key_value_heads\": 40,\n",
        "    \"pad_token_id\": 0,\n",
        "    \"pretraining_tp\": 1,\n",
        "    \"rms_norm_eps\": 1e-05,\n",
        "    \"rope_scaling\": None,\n",
        "    \"rope_theta\": 10000.0,\n",
        "    \"tie_word_embeddings\": False,\n",
        "    \"torch_dtype\": \"bfloat16\",\n",
        "    \"transformers_version\": \"4.45.2\",\n",
        "    \"use_cache\": True,\n",
        "    \"vocab_size\": 32000\n",
        "}\n",
        "\n",
        "# Указываем путь к файлу, в который будем сохранять конфигурацию\n",
        "json_file_path = '/home/jupyter/datasphere/project/ALMA/configs/conf.json'\n",
        "\n",
        "# Сохраняем конфигурацию в JSON файл\n",
        "with open(json_file_path, 'w') as json_file:\n",
        "    json.dump(config, json_file, indent=4)\n",
        "\n",
        "print(f\"Конфигурация сохранена в файл: {json_file_path}\")"
      ],
      "metadata": {
        "id": "gtR2fMAWKBzl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}