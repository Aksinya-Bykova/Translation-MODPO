{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU8f5O1E-pYL"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qqq > /dev/null\n",
        "import wandb\n",
        "\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "g6dTjXrk6m0u",
        "outputId": "8d23ebde-3b19-482c-ff08-ec3de6729cb8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g0IpFX7_8s7",
        "outputId": "7bc2f498-7a3e-4adc-bba1-41a6f678dfdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'modpo'...\n",
            "remote: Enumerating objects: 164, done.\u001b[K\n",
            "remote: Counting objects: 100% (164/164), done.\u001b[K\n",
            "remote: Compressing objects: 100% (105/105), done.\u001b[K\n",
            "remote: Total 164 (delta 67), reused 154 (delta 57), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (164/164), 58.64 KiB | 550.00 KiB/s, done.\n",
            "Resolving deltas: 100% (67/67), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -rf modpo\n",
        "\n",
        "!git clone https://github.com/ZHZisZZ/modpo.git > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# conda create -n modpo python=3.10\n",
        "# conda activate modpo\n",
        "# !pip install torch==2.1.0 --index-url https://download.pytorch.org/whl/cu118\n",
        "# !pip install -r requirements.txt\n",
        "# # (optional) pip install flash-attn==2.3.2 --no-build-isolation"
      ],
      "metadata": {
        "id": "zSJJUN92mCxm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab version for CPU, else modify code above\n",
        "\n",
        "# > /dev/null - to hide messages, but watch warning and errors\n",
        "# !pip install virtualenv > /dev/null\n",
        "# !virtualenv modpo > /dev/null\n",
        "\n",
        "# !source modpo/bin/activate > /dev/null\n",
        "\n",
        "# !pip install torch==2.1.0 --index-url https://download.pytorch.org/whl/cu118 > /dev/null\n",
        "\n",
        "# !pip install -r modpo/requirements.txt > /dev/null\n",
        "\n",
        "# !pip install flash-attn==2.3.2 --no-build-isolation > /dev/null"
      ],
      "metadata": {
        "id": "6uXQnHetmGMY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# another collab version for T4\n",
        "\n",
        "!pip uninstall -y torch torchaudio torchvision fsspec gcsfs datasets bigframes trl > /dev/null\n",
        "\n",
        "!pip install torch==2.4.1 torchaudio==2.4.1 torchvision==0.19.1 --index-url https://download.pytorch.org/whl/cu121 > /dev/null\n",
        "!pip install fsspec==2023.6.0 gcsfs>=2023.3.0 > /dev/null\n",
        "\n",
        "!pip install datasets==2.14.5 > /dev/null\n",
        "\n",
        "!pip install -r modpo/requirements.txt > /dev/null"
      ],
      "metadata": {
        "id": "pfdYS0btl0ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e75c42cb-0a79-4446-eecb-d7fbbcaf941e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping bigframes as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð°Ð²Ñ‚Ð¾Ñ€ÑÐºÐ¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸"
      ],
      "metadata": {
        "id": "hUr3gP6Xveo6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BKf1ZDg8vVaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.chdir(\"modpo\")\n",
        "\n",
        "# modified num_processes: 8 -> 1\n",
        "# edit your number!\n",
        "!PYTHONPATH=. accelerate launch --config_file scripts/accelerate_configs/multi_gpu.yaml --num_processes=1 \\\n",
        "    scripts/examples/dpo/dpo.py \\\n",
        "    --sft_model_name \"PKU-Alignment/alpaca-7b-reproduced\" \\\n",
        "    --prompt_template \"BEGINNING OF CONVERSATION: USER: {raw_prompt} ASSISTANT:\" \\\n",
        "    --dataset_name \"PKU-Alignment/PKU-SafeRLHF-10K-safer\" \\\n",
        "    --max_length 512 \\\n",
        "    --training_args.output_dir \"./output/PKU-Alignment/PKU-SafeRLHF-10K/modpo/rm/safer\" \\\n",
        "    --training_args.run_name \"PKU-Alignment/PKU-SafeRLHF-10K/modpo/rm/safer\" \\\n",
        "    --training_args.per_device_train_batch_size 6 \\\n",
        "    --training_args.per_device_eval_batch_size 6 \\\n",
        "    --training_args.gradient_accumulation_steps 2 \\\n",
        "    --training_args.learning_rate 5e-4 \\\n",
        "    --peft_config.r 64 \\\n",
        "    --peft_config.target_modules q_proj k_proj v_proj o_proj \\\n",
        "    --peft_config.lora_alpha 1 \\\n",
        "    --peft_config.lora_dropout 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBjVjXBNGGXs",
        "outputId": "33a40ac4-a3a2-4504-a97a-2d9371c51548"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "2024-10-14 12:09:21.885562: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-14 12:09:21.906607: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-14 12:09:21.914906: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-14 12:09:23.258710: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/tyro/_resolver.py:285: UserWarning: <class 'dict'> does not match any type in Union: [<class 'str'>, <class 'NoneType'>]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/tyro/_fields.py:333: UserWarning: The field base_model_name_or_path is annotated with type <class 'str'>, but the default value None has type <class 'NoneType'>. We'll try to handle this gracefully, but it may cause unexpected behavior.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/tyro/_fields.py:333: UserWarning: The field revision is annotated with type <class 'str'>, but the default value None has type <class 'NoneType'>. We'll try to handle this gracefully, but it may cause unexpected behavior.\n",
            "  warnings.warn(\n",
            "loading model...\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:437: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "Loading checkpoint shards: 100% 7/7 [01:06<00:00,  9.56s/it]\n",
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(32001, 4096, padding_idx=32000)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
            "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
            "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
            "          (act_fn): SiLUActivation()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm()\n",
            "        (post_attention_layernorm): LlamaRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)\n",
            ")\n",
            "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=64, target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'], lora_alpha=1, lora_dropout=0.0, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)\n",
            "mapping dataset to standard format...\n",
            "Map (num_proc=4): 100% 9000/9000 [00:01<00:00, 7005.09 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "mapping dataset to standard format...\n",
            "Map (num_proc=4): 100% 1000/1000 [00:00<00:00, 2268.78 examples/s]\n",
            "start training...\n",
            "dataset preprocessing...\n",
            "Map (num_proc=4):   0% 0/9000 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "Map (num_proc=4):  44% 4000/9000 [00:04<00:03, 1573.83 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n",
            "Map (num_proc=4): 100% 9000/9000 [00:11<00:00, 762.71 examples/s]\n",
            "Filter: 100% 9000/9000 [00:03<00:00, 2823.46 examples/s]\n",
            "Map (num_proc=4):   0% 0/1000 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.\n",
            "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
            "Map (num_proc=4): 100% 1000/1000 [00:01<00:00, 712.38 examples/s]\n",
            "Filter: 100% 1000/1000 [00:00<00:00, 2861.06 examples/s]\n",
            "train_dataset_retain: 0.9982222222222222\n",
            "eval_dataset_retain: 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m367121\u001b[0m (\u001b[33mhouse-666\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.18.3 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/modpo/wandb/run-20241014_121129-6nh3xxeh\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mPKU-Alignment/PKU-SafeRLHF-10K/modpo/rm/safer\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/house-666/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/house-666/huggingface/runs/6nh3xxeh\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:437: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "trainable params: 67,108,864 || all params: 6,805,532,672 || trainable%: 0.9860927459228279\n",
            "  0% 0/2247 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "[rank0]: Traceback (most recent call last):\n",
            "[rank0]:   File \"/content/modpo/scripts/examples/dpo/dpo.py\", line 125, in <module>\n",
            "[rank0]:     trainer.train()\n",
            "[rank0]:   File \"/content/modpo/src/trainer/dpo_trainer.py\", line 402, in train\n",
            "[rank0]:     initial_output = super().train(\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1591, in train\n",
            "[rank0]:     return inner_training_loop(\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1892, in _inner_training_loop\n",
            "[rank0]:     tr_loss_step = self.training_step(model, inputs)\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2776, in training_step\n",
            "[rank0]:     loss = self.compute_loss(model, inputs)\n",
            "[rank0]:   File \"/content/modpo/src/trainer/dpo_trainer.py\", line 518, in compute_loss\n",
            "[rank0]:     loss, metrics = self.get_batch_metrics(model, inputs, train_eval=\"train\")\n",
            "[rank0]:   File \"/content/modpo/src/trainer/dpo_trainer.py\", line 482, in get_batch_metrics\n",
            "[rank0]:     ) = self.forward(model, batch)\n",
            "[rank0]:   File \"/content/modpo/src/trainer/dpo_trainer.py\", line 451, in forward\n",
            "[rank0]:     all_logits = model(\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "[rank0]:     return self._call_impl(*args, **kwargs)\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "[rank0]:     return forward_call(*args, **kwargs)\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py\", line 1636, in forward\n",
            "[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py\", line 1454, in _run_ddp_forward\n",
            "[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "[rank0]:     return self._call_impl(*args, **kwargs)\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "[rank0]:     return forward_call(*args, **kwargs)\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 636, in forward\n",
            "[rank0]:     return model_forward(*args, **kwargs)\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 624, in __call__\n",
            "[rank0]:     return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\", line 43, in decorate_autocast\n",
            "[rank0]:     return func(*args, **kwargs)\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\", line 918, in forward\n",
            "[rank0]:     return self.base_model(\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "[rank0]:     return self._call_impl(*args, **kwargs)\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "[rank0]:     return forward_call(*args, **kwargs)\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\", line 94, in forward\n",
            "[rank0]:     return self.model.forward(*args, **kwargs)\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 1038, in forward\n",
            "[rank0]:     outputs = self.model(\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "[rank0]:     return self._call_impl(*args, **kwargs)\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "[rank0]:     return forward_call(*args, **kwargs)\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 925, in forward\n",
            "[rank0]:     layer_outputs = decoder_layer(\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "[rank0]:     return self._call_impl(*args, **kwargs)\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "[rank0]:     return forward_call(*args, **kwargs)\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 635, in forward\n",
            "[rank0]:     hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "[rank0]:     return self._call_impl(*args, **kwargs)\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "[rank0]:     return forward_call(*args, **kwargs)\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 361, in forward\n",
            "[rank0]:     query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
            "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 214, in apply_rotary_pos_emb\n",
            "[rank0]:     k_embed = (k * cos) + (rotate_half(k) * sin)\n",
            "[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 33.06 MiB is free. Process 187781 has 14.71 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 117.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "  0%|          | 0/2247 [00:02<?, ?it/s]\n",
            "[rank0]:[W1014 12:11:34.306158898 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
            "E1014 12:11:37.369000 138353530961920 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 15126) of binary: /usr/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 47, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 977, in launch_command\n",
            "    multi_gpu_launcher(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 646, in multi_gpu_launcher\n",
            "    distrib_run.run(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 892, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 133, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "scripts/examples/dpo/dpo.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2024-10-14_12:11:37\n",
            "  host      : 94c4e8d2d453\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 1 (pid: 15126)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMvWEMr3xZOiZS+TFafMg2s"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}